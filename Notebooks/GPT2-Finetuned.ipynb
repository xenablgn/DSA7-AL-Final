{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Author: *Abdul Hanan Bin Saeed***\n\n**Action Learning Team: 7**\n\n**Specialization: *DSA***","metadata":{}},{"cell_type":"markdown","source":"# FINETUNING GPT-2 (PEFT)","metadata":{}},{"cell_type":"markdown","source":"**Objective**","metadata":{}},{"cell_type":"markdown","source":"The objective of this notebook is to finetune GPT-2, using the technique; PEFT. We specifically finetune the model for achieving better performance in text-summarization tasks in the context of zero-shot learning.","metadata":{}},{"cell_type":"markdown","source":"**Dataset Info**","metadata":{}},{"cell_type":"markdown","source":"**DialogSum dataset**\n\nFor this purpose we utilize the DialogSum dataset.\n\nHow to access the Dataset: https://huggingface.co/datasets/neil-code/dialogsum-test\n\n* The dataset consists of 4 data fields; id, dialogue, summary, topic\n* The dataset has 3 splits: train, validation, and test. With 12 460, 500 and 1 500 records respectively.\n* The language of each record in this dataset is English.\n\nFurther information about the Datasets is provided on the Hugging Face website.","metadata":{}},{"cell_type":"markdown","source":"**Importing/Installing the relevant Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:08:39.585914Z","iopub.execute_input":"2024-06-27T12:08:39.586309Z","iopub.status.idle":"2024-06-27T12:09:23.175057Z","shell.execute_reply.started":"2024-06-27T12:08:39.586281Z","shell.execute_reply":"2024-06-27T12:09:23.174104Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:09:33.567946Z","iopub.execute_input":"2024-06-27T12:09:33.568775Z","iopub.status.idle":"2024-06-27T12:09:55.849013Z","shell.execute_reply.started":"2024-06-27T12:09:33.568738Z","shell.execute_reply":"2024-06-27T12:09:55.848049Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-27 12:09:41.246553: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-27 12:09:41.246679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-27 12:09:41.365891: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  n\n"},{"name":"stdout","text":"Token is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Disabling weights and biases**","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:10:02.962570Z","iopub.execute_input":"2024-06-27T12:10:02.963621Z","iopub.status.idle":"2024-06-27T12:10:02.967750Z","shell.execute_reply.started":"2024-06-27T12:10:02.963590Z","shell.execute_reply":"2024-06-27T12:10:02.966803Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Loading the Dataset**","metadata":{}},{"cell_type":"markdown","source":"We utilize the Dialogsum dataset, from Hugging Face.","metadata":{}},{"cell_type":"code","source":"dataset_name = \"neil-code/dialogsum-test\"\ndf = load_dataset(dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:10:05.732223Z","iopub.execute_input":"2024-06-27T12:10:05.732873Z","iopub.status.idle":"2024-06-27T12:10:12.257394Z","shell.execute_reply.started":"2024-06-27T12:10:05.732842Z","shell.execute_reply":"2024-06-27T12:10:12.256543Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaf2d3f5f4064a8092cb6351f9f4995f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9eccdc63d1b46c1aaedb2e0878fd145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2aed31084784250b2e9bc08eb219b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b68d21b93c243279e4f0d486ba82e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b8d55603774c80986694e43deb8ef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"584e5b96fab64663aba4243fc1a0f32c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2db90c1142947d5ad9e0024106976fe"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Having a better understanding of the structure of the dataset:**","metadata":{}},{"cell_type":"code","source":"df['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:17:16.383604Z","iopub.execute_input":"2024-06-27T12:17:16.384003Z","iopub.status.idle":"2024-06-27T12:17:16.391681Z","shell.execute_reply.started":"2024-06-27T12:17:16.383973Z","shell.execute_reply":"2024-06-27T12:17:16.390730Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}]},{"cell_type":"markdown","source":"From the above output, we can infer that each record in this dataset consists of some dialogues between people, a summary of the conversation and a topic directing towards the main central idea of the conversation.","metadata":{}},{"cell_type":"markdown","source":"**Specifying the Quantization to be performed**","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:17:19.334068Z","iopub.execute_input":"2024-06-27T12:17:19.334464Z","iopub.status.idle":"2024-06-27T12:17:19.340663Z","shell.execute_reply.started":"2024-06-27T12:17:19.334432Z","shell.execute_reply":"2024-06-27T12:17:19.339748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Loading the GPT-2 Model**","metadata":{}},{"cell_type":"markdown","source":"In this section, we load the pre-trained GPT-2 Model from Hugging Face.\n\n**About the Model:** *GPT-2 is developed by OpenAI, based on the Transformer architecture influenced by the paper “Attention is all you need”. The model can be used for various applications; text-generation being the most suitable. The model makes use of self-attention mechanisms for a better understanding of relationships and dependencies between the input text. The number of parameters vary for each variation of GPT-2, however the model used for this study is the smallest version of GPT-2, with 124M parameters. Moreover, this model is trained on WebText in a self-supervised manner. This model can exhibit biases in its behaviour and can produce limited results.* ","metadata":{}},{"cell_type":"code","source":"model_name='openai-community/gpt2'\ndevice_map = {\"\": 0}\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:17:24.645457Z","iopub.execute_input":"2024-06-27T12:17:24.646095Z","iopub.status.idle":"2024-06-27T12:17:47.505400Z","shell.execute_reply.started":"2024-06-27T12:17:24.646065Z","shell.execute_reply":"2024-06-27T12:17:47.504645Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb56d696ca04ddea6b49ddeef4c608e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff4d4e372744c26bff9833493b5f4ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a418a7b4eaa412ea6476eb77f4ee42b"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Setting up the Tokenizer**\n\nWe incorporate left-padding for a better memory-usage during the training phase.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:17:53.350676Z","iopub.execute_input":"2024-06-27T12:17:53.351451Z","iopub.status.idle":"2024-06-27T12:17:55.775785Z","shell.execute_reply.started":"2024-06-27T12:17:53.351421Z","shell.execute_reply":"2024-06-27T12:17:55.774979Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de0a8f4154b4bcaa66ed0f1dbe1e80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb4507dd2f049ebba686fbbde4710ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08630327c64b48d3a14533c70114a0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2257454f160a49f1bd8994aac9800685"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Evaluating the base model**","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed, pipeline\nseed = 42\nset_seed(seed)\n\ngen = pipeline(\"text-generation\", model=original_model, tokenizer=tokenizer)\n\nindex = 10\n\nprompt = df['test'][index]['dialogue']\nsummary = df['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(formatted_prompt, max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\n#print(res[0])\noutput = res[0]['generated_text'].split('Output:\\n')[1].strip() if 'Output:\\n' in res[0]['generated_text'] else \"No output generated.\"\n\n# Print the results\ndash_line = '-' * 100\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:00.045731Z","iopub.execute_input":"2024-06-27T12:18:00.046494Z","iopub.status.idle":"2024-06-27T12:18:05.052580Z","shell.execute_reply.started":"2024-06-27T12:18:00.046461Z","shell.execute_reply":"2024-06-27T12:18:05.051659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n----------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n----------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: Brian, I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person2#: Brian, may I have a pleasure to have a dance with you?\n#Person1#: Brian, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person2#: Ok.\n#Person1#: Brian, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\nCPU times: user 4.27 s, sys: 125 ms, total: 4.4 s\nWall time: 5 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Dataset Pre-processing Stage**","metadata":{}},{"cell_type":"markdown","source":"To make this dataset suitable for model training, we define functions for data preprocessing where text-samples from the dataset are formatted for the model and suitable prompts are constructed. The formatted prompt contains INTRO_BLURB, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY, where INSTRUCTION_KEY being \"### Instruct: Summarize the below conversation.\" Another function ensures that the input sequence is compatible with the length that the model can handle, after checking the model configuration and setting the default value to 1024. Similarly, the data in batches is tokenized and a longer sequence of samples is truncated. The dataset is also shuffled according to the provided seed. Consequently, each sample of data is tokenized, formatted and pre-processed with respect to the model being fine tuned.","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:10.837971Z","iopub.execute_input":"2024-06-27T12:18:10.838817Z","iopub.status.idle":"2024-06-27T12:18:10.845343Z","shell.execute_reply.started":"2024-06-27T12:18:10.838784Z","shell.execute_reply":"2024-06-27T12:18:10.844350Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:15.909545Z","iopub.execute_input":"2024-06-27T12:18:15.910352Z","iopub.status.idle":"2024-06-27T12:18:15.922571Z","shell.execute_reply.started":"2024-06-27T12:18:15.910305Z","shell.execute_reply":"2024-06-27T12:18:15.921620Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, df['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, df['validation'])","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:22.396405Z","iopub.execute_input":"2024-06-27T12:18:22.397119Z","iopub.status.idle":"2024-06-27T12:18:34.767219Z","shell.execute_reply.started":"2024-06-27T12:18:22.397083Z","shell.execute_reply":"2024-06-27T12:18:34.766269Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found max lenth: 1024\n1024\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61845b3dcd8e422d8cabaaad0aca6861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5e811cc4944dee8dc44f816bf27fb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322a6bfd8ce8491da4999947f2917cd3"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691e93a37373440aa6d836f2e926b8f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a0f71443ea4874834ba2f8814ce670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e9b3eff4f60434a8bb4e7d42a3ff1d3"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Initializing the model for QLoRA**","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:38.995704Z","iopub.execute_input":"2024-06-27T12:18:38.996342Z","iopub.status.idle":"2024-06-27T12:18:39.000584Z","shell.execute_reply.started":"2024-06-27T12:18:38.996311Z","shell.execute_reply":"2024-06-27T12:18:38.999598Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"original_model = prepare_model_for_kbit_training(original_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:42.101940Z","iopub.execute_input":"2024-06-27T12:18:42.102314Z","iopub.status.idle":"2024-06-27T12:18:42.111969Z","shell.execute_reply.started":"2024-06-27T12:18:42.102284Z","shell.execute_reply":"2024-06-27T12:18:42.111183Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Getting a better picture of GPT-2's architecture**","metadata":{}},{"cell_type":"code","source":"print(original_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:45.375955Z","iopub.execute_input":"2024-06-27T12:18:45.376336Z","iopub.status.idle":"2024-06-27T12:18:45.381911Z","shell.execute_reply.started":"2024-06-27T12:18:45.376304Z","shell.execute_reply":"2024-06-27T12:18:45.380981Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Defining the LoRA config for Fine-tuning GPT-2**","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'c_attn',\n        'c_proj',\n        'c_fc'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\npeft_model = get_peft_model(original_model, config)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:51.308294Z","iopub.execute_input":"2024-06-27T12:18:51.308679Z","iopub.status.idle":"2024-06-27T12:18:51.433574Z","shell.execute_reply.started":"2024-06-27T12:18:51.308649Z","shell.execute_reply":"2024-06-27T12:18:51.432660Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Checking the Trainable parameters**","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:18:56.839155Z","iopub.execute_input":"2024-06-27T12:18:56.839514Z","iopub.status.idle":"2024-06-27T12:18:56.848105Z","shell.execute_reply.started":"2024-06-27T12:18:56.839488Z","shell.execute_reply":"2024-06-27T12:18:56.847178Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"trainable model parameters: 4718592\nall model parameters: 86691072\npercentage of trainable model parameters: 5.44%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training Process**","metadata":{}},{"cell_type":"markdown","source":"To finetune GPT-2 we make use of PEFT with LoRA, with details defined as follows:\n\n* LoRA Parameters for GPT-2\nThe task for which the model is intended for is CAUSAL_LM. For this study we specify the rank ‘r’, and ‘lora_alpha’ to 32. We set the target modules as 'c_attn', 'c_proj' and 'c_fc' namely Context Attention, Context Projection and Context Fully Connected, to undergo low-rank adaption as these modules are the most crucial in a transformer-based architecture used for tasks like summarization and generation  (language modelling), where these layers enable the model to learn deeply about the sequential data and process it. To reduce the computational complexity, we set ‘bias’ = ‘none’. Furthermore, to avoid overfitting, we set lora_dropout=0.05.\n\nIn the training configuration, we set-up gradient accumulation and checkpointing. \n","metadata":{}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:06.670026Z","iopub.execute_input":"2024-06-27T12:19:06.670472Z","iopub.status.idle":"2024-06-27T12:19:06.708293Z","shell.execute_reply.started":"2024-06-27T12:19:06.670441Z","shell.execute_reply":"2024-06-27T12:19:06.707423Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Evaluation**","metadata":{}},{"cell_type":"markdown","source":"* Loading the pre-trained model and setting up the tokenizer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"openai-community/gpt2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:23.968042Z","iopub.execute_input":"2024-06-27T12:19:23.968711Z","iopub.status.idle":"2024-06-27T12:19:25.397898Z","shell.execute_reply.started":"2024-06-27T12:19:23.968682Z","shell.execute_reply":"2024-06-27T12:19:25.396944Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:30.682558Z","iopub.execute_input":"2024-06-27T12:19:30.683331Z","iopub.status.idle":"2024-06-27T12:19:31.020856Z","shell.execute_reply.started":"2024-06-27T12:19:30.683297Z","shell.execute_reply":"2024-06-27T12:19:31.019845Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"* Loading the saved finetuned model","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training-1719345153/checkpoint-725\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:37.985747Z","iopub.execute_input":"2024-06-27T12:19:37.986634Z","iopub.status.idle":"2024-06-27T12:19:38.146233Z","shell.execute_reply.started":"2024-06-27T12:19:37.986595Z","shell.execute_reply":"2024-06-27T12:19:38.145301Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"* Inference with the PEFT model","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\ngen = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)\n\ndialogue = df['test'][index]['dialogue']\nsummary = df['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(prompt,max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\npeft_model_output = peft_model_res[0]['generated_text'].split('Output:\\n')[1].strip() if 'Output:\\n' in res[0]['generated_text'] else \"No output generated.\"\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('###')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:19:42.888601Z","iopub.execute_input":"2024-06-27T12:19:42.889277Z","iopub.status.idle":"2024-06-27T12:19:48.848744Z","shell.execute_reply.started":"2024-06-27T12:19:42.889243Z","shell.execute_reply":"2024-06-27T12:19:48.847788Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nBrian is happy that Brian has a good time.\n\n\nCPU times: user 5.94 s, sys: 4.11 ms, total: 5.94 s\nWall time: 5.95 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ROUGE Metric Evaluation**","metadata":{}},{"cell_type":"markdown","source":"In this step we compare the performance of our finetuned model with the original base model, using ROUGE metric evaluation. We perform inference on the same dataset on which the model is finetuned, but on the 'test' split.","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:20:05.583500Z","iopub.execute_input":"2024-06-27T12:20:05.584277Z","iopub.status.idle":"2024-06-27T12:20:06.967880Z","shell.execute_reply.started":"2024-06-27T12:20:05.584244Z","shell.execute_reply":"2024-06-27T12:20:06.967134Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = df['test'][0:10]['dialogue']\nhuman_baseline_summaries = df['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\noriginal_model_gen = pipeline(\"text-generation\", model=original_model, tokenizer=tokenizer)\npeft_model_gen = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)\n\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = original_model_gen(prompt, max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\n    original_model_text_output = original_model_res[0]['generated_text'].split('Output:\\n')[1].strip() if 'Output:\\n' in res[0]['generated_text'] else \"No output generated.\"\n    \n    peft_model_res = peft_model_gen(prompt,max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\n    peft_model_output = peft_model_res[0]['generated_text'].split('Output:\\n')[1].strip() if 'Output:\\n' in res[0]['generated_text'] else \"No output generated.\"\n    print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('###')\n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf_metrics = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf_metrics","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:20:09.670399Z","iopub.execute_input":"2024-06-27T12:20:09.670766Z","iopub.status.idle":"2024-06-27T12:21:46.862782Z","shell.execute_reply.started":"2024-06-27T12:20:09.670737Z","shell.execute_reply":"2024-06-27T12:21:46.861893Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"},{"name":"stdout","text":"#Person1# asks #Person2# to take a dictation for #Person2#. #Person2# tells #Person1# #Person1# will have to take a dictation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the\nMr. Dawson wants to take a dictation from Ms. Dawson and sends the memo to all employees.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End\n#Person1# asks Ms. Dawson to take a dictation for #Person1# to take a dictation for her office. #Person2# asks for the memo and sends it to #Person1#. #Person1# wants to get the memo typed up and distributed before 4 pm.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation\n#Person1# tells #Person2# that #Person2# will not quit driving to work. #Person2# thinks #Person2# might quit driving to work if #Person1# decides to quit driving to work.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation\n#Person2# thinks #Person1# should try to find a different route to get home. #Person1# thinks #Person2# should try to find a different route to get home.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation\n#Person1# tells #Person1# #Person2#'s car is causing the pollution problem in #Person2#'s city. #Person2# suggests stopping driving to work.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation\nKate tells #Person2# Masha and Hero are getting divorced. #Person1# thinks it's a big surprise, but Masha and Hero are getting divorced anyway.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End\nKate is worried about the kids. She is afraid that Masha and Hero will divorce soon.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation\nKate tells #Person1# about the divorce and the kids.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the\nBrian and #Person2# have a good time and enjoy the party.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n### End the conversation.\n\n###\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  #Person1#: *PERSON1*\\n#Person2#: *PERSON2*\\n#P...   \n1  As always, the program is available on the Int...   \n2  #Person1#: I will take a dictation for you.\\n#...   \n3           #Person1#: You're going to quit driving.   \n4  #Person1#: It's not my place to tell you what ...   \n5  #Person1#: What is your favorite part of the c...   \n6                #Person2#: I don't think they will.   \n7   #Person1#: That's it.\\n#Person2#: You are right.   \n8  The final chapter of this book was written in ...   \n9  #Person1#: I was just feeling very sleepy, I'm...   \n\n                                peft_model_summaries  \n0  #Person1# asks #Person2# to take a dictation f...  \n1  Mr. Dawson wants to take a dictation from Ms. ...  \n2  #Person1# asks Ms. Dawson to take a dictation ...  \n3  #Person1# tells #Person2# that #Person2# will ...  \n4  #Person2# thinks #Person1# should try to find ...  \n5  #Person1# tells #Person1# #Person2#'s car is c...  \n6  Kate tells #Person2# Masha and Hero are gettin...  \n7  Kate is worried about the kids. She is afraid ...  \n8  Kate tells #Person1# about the divorce and the...  \n9  Brian and #Person2# have a good time and enjoy...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1#: *PERSON1*\\n#Person2#: *PERSON2*\\n#P...</td>\n      <td>#Person1# asks #Person2# to take a dictation f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>As always, the program is available on the Int...</td>\n      <td>Mr. Dawson wants to take a dictation from Ms. ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>#Person1#: I will take a dictation for you.\\n#...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>#Person1#: You're going to quit driving.</td>\n      <td>#Person1# tells #Person2# that #Person2# will ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>#Person1#: It's not my place to tell you what ...</td>\n      <td>#Person2# thinks #Person1# should try to find ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>#Person1#: What is your favorite part of the c...</td>\n      <td>#Person1# tells #Person1# #Person2#'s car is c...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>#Person2#: I don't think they will.</td>\n      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>#Person1#: That's it.\\n#Person2#: You are right.</td>\n      <td>Kate is worried about the kids. She is afraid ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>The final chapter of this book was written in ...</td>\n      <td>Kate tells #Person1# about the divorce and the...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>#Person1#: I was just feeling very sleepy, I'm...</td>\n      <td>Brian and #Person2# have a good time and enjoy...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"* Statistics","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n\nprint(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-27T12:21:58.490713Z","iopub.execute_input":"2024-06-27T12:21:58.491068Z","iopub.status.idle":"2024-06-27T12:22:00.886003Z","shell.execute_reply.started":"2024-06-27T12:21:58.491042Z","shell.execute_reply":"2024-06-27T12:22:00.884962Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ece285ceab6445c88b224e33471ddc4"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.15140897852634483, 'rouge2': 0.016760347470618293, 'rougeL': 0.12892774470287083, 'rougeLsum': 0.14493243920342874}\nPEFT MODEL:\n{'rouge1': 0.352963550227572, 'rouge2': 0.08746638678239227, 'rougeL': 0.26950756720566904, 'rougeLsum': 0.2715624076597606}\nAbsolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 20.16%\nrouge2: 7.07%\nrougeL: 14.06%\nrougeLsum: 12.66%\n","output_type":"stream"}]}]}