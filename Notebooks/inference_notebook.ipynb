{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/xenanurbilgin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from Modules.inference import *\n",
    "from Modules.utils import *\n",
    "import textwrap\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Data:\n",
    "\n",
    "prompt_template = COT_PROMT  \n",
    "dataset_bbc = load_dataset('gopalkalpande/bbc-news-summary', split='train')  # BBC News dataset\n",
    "dataset_bbc_train_test = dataset_bbc.train_test_split(test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory created: cache/Google-Fine-Tuned\n",
      "Loading Google Fine Tuned model from cache: cache/Google-Fine-Tuned\n"
     ]
    }
   ],
   "source": [
    "## Load Model:\n",
    "\n",
    "model_type = 'Google-Fine-Tuned'# or 'Google-Base', 'GOOGLE-PEFT-LORA', 'T5-Double-Tuned', 'Google-Fine-Tuned'\n",
    "model, tokenizer = load_model_and_tokenizer(model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (788 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "## Data Sample:\n",
    "inputs = dataset_bbc[100:101]['Articles']\n",
    "human_baseline_summaries = dataset_bbc[100:101]['Summaries']\n",
    "\n",
    "## Make Prediction\n",
    "model_output, rouge_scores = summarize_and_evaluate(model, tokenizer, inputs, human_baseline_summaries, model_type, prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text</th>\n",
       "      <th>Human Baseline Summaries</th>\n",
       "      <th>Model Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blair dismisses quit claim report..Tony Blair ...</td>\n",
       "      <td>Former welfare minister Frank Field MP said th...</td>\n",
       "      <td>Mr Preston told BBC News My understanding is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Input Text  \\\n",
       "0  Blair dismisses quit claim report..Tony Blair ...   \n",
       "\n",
       "                            Human Baseline Summaries  \\\n",
       "0  Former welfare minister Frank Field MP said th...   \n",
       "\n",
       "                                     Model Summaries  \n",
       "0  Mr Preston told BBC News My understanding is t...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Former welfare minister Frank Field MP said the prime minister should sack Mr\n",
      "Brown, but did not believe Mr Blair was strong enough to do so.Mr Blair said the\n",
      "claims were \"reheated from six months ago\" and that he was concentrating on\n",
      "running the country.The Liberal Democrat parliamentary chairman Matthew Taylor\n",
      "said the personal ambition of Mr Blair and Mr Brown was \"getting in the way of\n",
      "good government\".According to Mr Peston the prime minister said: \"Help me to get\n",
      "through the year and I will then stand down.\"According to a new book, Brown's\n",
      "Britain, Mr Blair went back on a pledge to make way for Mr Brown after Cabinet\n",
      "allies intervened in June 2004.Mr Blair said: \"I've dealt with this six months\n",
      "ago.And that at a dinner hosted by Deputy Prime Minister John Prescott he told\n",
      "Mr Brown of his intention to stand down.During the interview Mr Blair also said\n",
      "the former home secretary David Blunkett would play a \"big role\" at the general\n",
      "election.Tory leader Michael Howard accused the prime minister and Mr Brown of\n",
      "\"squabbling like schoolboys\".But, in a wide-ranging BBC interview covering\n",
      "issues such as the Asian tsunami disaster, the Middle East peace process and\n",
      "Northern Ireland, Mr Blair said: \"When you get to the top in politics you get\n",
      "this huge swell around you.But the prime minister said he had discussed these\n",
      "claims with the chancellor and dismissed them as a \"load of nonsense\".Tony Blair\n",
      "has dismissed reports he told Gordon Brown he would quit before the next general\n",
      "election.\n"
     ]
    }
   ],
   "source": [
    "long_text = model_output.iloc[0, 0]\n",
    "print_wrapped_text(long_text, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Blair said the claims were reheated from six months ago and that he was\n",
      "concentrating on running the country Mr Blair said I've dealt with this six\n",
      "months ago I said then you don't do deals over jobs like this you somewhat both\n",
      "of us are actually concentrating on are the issues that concern the country The\n",
      "book by Sunday Telegraph journalist Robert Preston and serialized in the\n",
      "newspaper said the pair had mutual animosity and contempt for each other. Mr\n",
      "Preston The prime minister said he had discussed these claims with the\n",
      "chancellor and dismissed them as a load of nonsense. Following the interview Mr\n",
      "Blair also said the former home secretary David Blanket would play a big role at\n",
      "the general election.\n"
     ]
    }
   ],
   "source": [
    "long_text = model_output.iloc[0, 1]\n",
    "print_wrapped_text(long_text, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.48484848484848475, 'rouge2': 0.350253807106599, 'rougeL': 0.33333333333333337, 'rougeLsum': 0.33333333333333337}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inference(prompt_template, model_type, user_input, user_input_type) -> [pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Generates summaries and computes ROUGE scores based on user input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data from CSV or use the DataFrame directly\n",
    "    if user_input_type == \"csv\":\n",
    "        # Load CSV file into DataFrame\n",
    "        df = pd.read_csv(user_input)\n",
    "        inputs = df.iloc[:, 0].tolist()  # Assuming the first column contains the text\n",
    "        human_baseline_summaries = df.iloc[:, 1].tolist() if df.shape[1] > 1 else []\n",
    "    elif user_input_type == \"list\":\n",
    "        # Directly use the input list\n",
    "        inputs = user_input\n",
    "        human_baseline_summaries = []\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported user_input_type. Use 'csv' or 'list'.\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_type)\n",
    "\n",
    "    # Generate summaries and compute ROUGE scores\n",
    "    model_output, rouge_scores = summarize_and_evaluate(model, tokenizer, inputs, human_baseline_summaries, prompt_template)\n",
    "    \n",
    "    return model_output, rouge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "import language_tool_python\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ['HUGGINGFACEHUB_API_TOKEN'])\n",
    "\n",
    "# Initialize spell checker and grammar tool\n",
    "spell = SpellChecker()\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "\n",
    "def cache_directory(model_name):\n",
    "    \"\"\"\n",
    "    Create a cache directory based on the model name.\n",
    "    \"\"\"\n",
    "    cache_dir = os.path.join(\"cache\", model_name.replace(\"/\", \"_\"))\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    print(f\"Cache directory created: {cache_dir}\")\n",
    "    return cache_dir\n",
    "\n",
    "def load_model_and_tokenizer(model_type, version=None, torch_dtype=torch.bfloat16, is_trainable=False):\n",
    "    \"\"\"\n",
    "    Load model and tokenizer based on the given parameters with caching.\n",
    "    \"\"\"\n",
    "    if model_type == 'Google-Base':\n",
    "        model_name = \"google/flan-t5-base\"\n",
    "        cache_dir = cache_directory(model_name)\n",
    "        print(f\"Loading Google Base model from cache: {cache_dir}\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch_dtype, cache_dir=cache_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    elif model_type == 'GOOGLE-PEFT-LORA':\n",
    "        model_name = \"google/flan-t5-base\"\n",
    "        peft_path = \"../Models/GOOGLE-PEFT-LoRA-COMPLEX\"\n",
    "        cache_dir = cache_directory(model_name)\n",
    "        print(f\"Loading base model for PEFT from cache: {cache_dir}\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch_dtype, cache_dir=cache_dir)\n",
    "        print(f\"Loading PEFT model from: {peft_path}\")\n",
    "        model = PeftModel.from_pretrained(base_model, peft_path, torch_dtype=torch_dtype, is_trainable=is_trainable)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    elif model_type == 'T5-Double-Tuned':\n",
    "        model_path = \"../Models/T52-BCN-DIALOG/checkpoint-3115\"\n",
    "        tokenizer_path = \"../Models/T52-BCN-DIALOG\"\n",
    "        cache_dir = cache_directory(\"T5-Double-Tuned\")\n",
    "        print(f\"Loading T5 Double Tuned model from cache: {cache_dir}\")\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch_dtype, cache_dir=cache_dir)\n",
    "        tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, cache_dir=cache_dir)\n",
    "    \n",
    "    elif model_type == 'Google-Fine-Tuned':\n",
    "        model_name = \"google/flan-t5-base\"\n",
    "        model_path = \"../Models/GOOGLE-BCN/checkpoint-2225\"\n",
    "        tokenizer_path = \"../Models/GOOGLE-BCN\"\n",
    "        cache_dir = cache_directory(\"Google-Fine-Tuned\")\n",
    "        print(f\"Loading Google Fine Tuned model from cache: {cache_dir}\")\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch_dtype, cache_dir=cache_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type. Use 'Google-Base', 'GOOGLE-PEFT-LORA', 'T5-Double-Tuned', or 'Google-Fine-Tuned'.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def remove_redundant_phrases(summary):\n",
    "    \"\"\"\n",
    "    Removes redundant sentences from a summary.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(summary)\n",
    "    unique_sentences = list(dict.fromkeys(sentences))  # Remove duplicates while preserving order\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "def correct_grammar_and_spelling(summary):\n",
    "    \"\"\"\n",
    "    Corrects grammar and spelling errors in a summary.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(summary)\n",
    "    corrected_words = [spell.correction(word) if word.lower() in spell else word for word in words]\n",
    "    corrected_summary = ' '.join(corrected_words)\n",
    "    corrected_summary = tool.correct(corrected_summary)  # Use LanguageTool for grammar correction\n",
    "    return corrected_summary\n",
    "\n",
    "def trim_unnecessary_information(summary):\n",
    "    \"\"\"\n",
    "    Trims unnecessary phrases like 'In conclusion', 'To summarize', 'In summary' from a summary.\n",
    "    \"\"\"\n",
    "    fillers = [\"In conclusion\", \"To summarize\", \"In summary\"]\n",
    "    for filler in fillers:\n",
    "        summary = summary.replace(filler, '')\n",
    "    return summary.strip()\n",
    "\n",
    "def post_process_summary(summary):\n",
    "    \"\"\"\n",
    "    Post-processes a summary by removing redundancy, correcting grammar and spelling, and trimming unnecessary information.\n",
    "    \"\"\"\n",
    "    summary = remove_redundant_phrases(summary)\n",
    "    summary = correct_grammar_and_spelling(summary)\n",
    "    summary = trim_unnecessary_information(summary)\n",
    "    return summary\n",
    "\n",
    "def compute_rouge_for_model(predictions, references):\n",
    "    \"\"\"\n",
    "    Computes ROUGE scores between predicted summaries and reference summaries.\n",
    "    \"\"\"\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references[:len(predictions)],  # Ensure references match predictions length\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_summary(model, tokenizer, input_ids):\n",
    "    \"\"\"\n",
    "    Generates a summary using a T5 sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=100,\n",
    "            temperature=0.2,\n",
    "            num_beams=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "\n",
    "def summarize_and_evaluate(model, tokenizer, input_texts, human_baseline_summaries, prompt_template):\n",
    "    \"\"\"\n",
    "    Summarizes texts using a single model and evaluates the summaries.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for input_text in input_texts:\n",
    "        prompt = prompt_template.format(input=input_text)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        summary = generate_summary(model, tokenizer, input_ids)\n",
    "        summaries.append(post_process_summary(summary))\n",
    "\n",
    "    # Create DataFrame for results\n",
    "    df = pd.DataFrame({\n",
    "        \"human_baseline_summaries\": human_baseline_summaries if human_baseline_summaries else [\"\"] * len(input_texts),\n",
    "        \"model_summaries\": summaries\n",
    "    })\n",
    "    \n",
    "    # Compute ROUGE scores only if human_baseline_summaries is not empty\n",
    "    rouge_scores = {}\n",
    "    if human_baseline_summaries:\n",
    "        rouge_scores = compute_rouge_for_model(summaries, human_baseline_summaries)\n",
    "\n",
    "    return df, rouge_scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
